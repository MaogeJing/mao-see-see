#!/usr/bin/env python3
"""
å®éªŒè„šæœ¬ï¼šæµ‹è¯•å°çº¢ä¹¦ç¬”è®°åˆ—è¡¨æŠ“å–åŠŸèƒ½
"""

from DrissionPage import Chromium
import time
import json
from datetime import datetime
import sys
import os

# æ·»åŠ æ•°æ®ç»“æ„è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'data_structures'))
from xiaohongshu_models import XiaohongshuNote, XiaohongshuSearchResponse, save_notes_to_json

def test_note_list_capture():
    """æµ‹è¯•ç¬”è®°åˆ—è¡¨æŠ“å–"""
    print("ğŸ” å®éªŒå¼€å§‹ï¼šå°çº¢ä¹¦ç¬”è®°åˆ—è¡¨æŠ“å–")
    print("=" * 50)

    try:
        # 1. å¯åŠ¨æŒä¹…åŒ–æµè§ˆå™¨
        print("ğŸ“ æ­¥éª¤1: å¯åŠ¨æµè§ˆå™¨...")
        browser = Chromium(9933)
        tab = browser.get_tab(0)
        print("âœ… æµè§ˆå™¨å¯åŠ¨æˆåŠŸ")

        # 2. å¯¼èˆªåˆ°å°çº¢ä¹¦
        if 'xiaohongshu.com' not in tab.url: # type: ignore
            print("ğŸ“ æ­¥éª¤2: å¯¼èˆªåˆ°å°çº¢ä¹¦...")
            tab.get('https://www.xiaohongshu.com/')
            time.sleep(5)
            print("âœ… å·²æ‰“å¼€å°çº¢ä¹¦")

        # 3. å¯åŠ¨ç½‘ç»œç›‘å¬
        print("ğŸ“ æ­¥éª¤3: å¯åŠ¨ç½‘ç»œç›‘å¬...")
        print(tab.listen.start('xiaohongshu.com'))
        print("âœ… ç½‘ç»œç›‘å¬å·²å¯åŠ¨")

        # 4. å¼•å¯¼ç”¨æˆ·æ“ä½œ
        print("\n" + "=" * 50)
        print("ğŸ“ è¯·åœ¨æµè§ˆå™¨ä¸­è¿›è¡Œä»¥ä¸‹æ“ä½œï¼š")
        print("ğŸ¯ é‡ç‚¹ï¼šæœç´¢ä»»æ„å…³é”®è¯ï¼ˆå¦‚ï¼šPythonã€ç¼–ç¨‹ã€ç¾é£Ÿç­‰ï¼‰")
        print("ğŸ’¡ å…³é”®æ¥å£ï¼šhttps://edith.xiaohongshu.com/api/sns/web/v1/search/notes")
        print("ğŸ“± æ­¥éª¤ï¼š")
        print("   1. ç‚¹å‡»æœç´¢æ¡†")
        print("   2. è¾“å…¥å…³é”®è¯")
        print("   3. ç‚¹å‡»æœç´¢æˆ–æŒ‰å›è½¦")
        print("   4. è§‚å¯Ÿç¨‹åºè¾“å‡ºï¼ˆä¼šæ˜¾ç¤ºğŸ”¥æ ‡è¯†çš„å…³é”®æ¥å£ï¼‰")
        print("   5. æ»šåŠ¨é¡µé¢æŸ¥çœ‹æ›´å¤šç¬”è®°")
        print("   6. æŒ‰ Ctrl+C ç»“æŸæµ‹è¯•")
        print("=" * 50)

        # 5. å¼€å§‹ç›‘å¬å’Œæ•è·
        captured_notes = []
        last_request_count = 0

        print("ğŸ”„ å¼€å§‹ç›‘å¬ç½‘ç»œè¯·æ±‚...")
        print("ğŸ’¡ æç¤ºï¼šè¯·åœ¨æµè§ˆå™¨ä¸­æœç´¢å…³é”®è¯")
        print("ğŸ’¡ ç¨‹åºä¼šæ¯3ç§’æ£€æŸ¥ä¸€æ¬¡æ–°çš„ç½‘ç»œè¯·æ±‚")
        print("ğŸ’¡ æŒ‰ Ctrl+C ç»“æŸæµ‹è¯•")
        print("-" * 50)

        while True:
            try:
                # ä½¿ç”¨è¶…æ—¶æ–¹å¼é¿å…é˜»å¡
                new_requests = []

                # å°è¯•è·å–æ–°çš„ç½‘ç»œè¯·æ±‚ï¼Œè®¾ç½®çŸ­æš‚è¶…æ—¶
                try:
                    for packet in tab.listen.steps(timeout=1):
                        new_requests.append(packet)
                except:
                    # è¶…æ—¶æ˜¯æ­£å¸¸çš„ï¼Œç»§ç»­å¾ªç¯
                    pass

                if new_requests:
                    print(f"\nğŸ“Š æ•è·åˆ° {len(new_requests)} ä¸ªæ–°è¯·æ±‚")

                    for packet in new_requests:
                        try:
                            print(f"ğŸŒ {packet.method} {packet.url[:80]}...")

                            # ä¸“é—¨å¤„ç†æœç´¢ç¬”è®°API
                            if 'edith.xiaohongshu.com/api/sns/web/v1/search/notes' in packet.url:
                                print(f"\nğŸ”¥ å‘ç°å…³é”®æ¥å£ï¼ç¬”è®°åˆ—è¡¨API")

                                # å°è¯•è·å–çŠ¶æ€ç 
                                status = 'æœªçŸ¥'
                                try:
                                    status = packet.response.status if packet.response else 'æ— å“åº”'
                                except:
                                    pass
                                print(f"   çŠ¶æ€: {status}")

                                # ç›´æ¥å¤„ç†APIå“åº”
                                if hasattr(packet, 'response') and packet.response:
                                    try:
                                        if hasattr(packet.response, 'body') and packet.response.body:
                                            notes = process_search_api_response(packet.response.body)
                                            if notes:
                                                captured_notes.extend(notes)
                                                print(f"âœ… æå–åˆ° {len(notes)} ä¸ªç¬”è®°")
                                                for note in notes[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                                                    print(f"   ğŸ“ {note.get('title', 'æ— æ ‡é¢˜')[:50]}...")
                                    except Exception as e:
                                        print(f"âš ï¸ å¤„ç†å“åº”å¤±è´¥: {str(e)}")

                            else:
                                # å…¶ä»–ç›¸å…³è¯·æ±‚
                                if any(keyword in packet.url for keyword in ['xiaohongshu', 'search', 'note']):
                                    print(f"   ğŸ” ç›¸å…³è¯·æ±‚")

                        except Exception as e:
                            print(f"âš ï¸ å¤„ç†æ•°æ®åŒ…å¼‚å¸¸: {str(e)}")

                # æ˜¾ç¤ºçŠ¶æ€
                if captured_notes:
                    print(f"ğŸ“ˆ å½“å‰æ•è·ç¬”è®°: {len(captured_notes)} ä¸ª")

                time.sleep(3)  # æ¯3ç§’æ£€æŸ¥ä¸€æ¬¡

            except KeyboardInterrupt:
                print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
                break
            except Exception as e:
                print(f"âš ï¸ ç›‘æ§å¼‚å¸¸: {str(e)}")
                time.sleep(2)

        # 6. ä¿å­˜ç»“æœ
        if captured_notes:
            save_captured_notes(captured_notes)
            print(f"\nğŸ’¾ å·²ä¿å­˜ {len(captured_notes)} ä¸ªç¬”è®°ä¿¡æ¯")

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()

def analyze_request(request):
    """åˆ†æè¯·æ±‚ç±»å‹"""
    url = request.url.lower()

    result = {
        'url': request.url,
        'url_short': request.url.split('/')[-1][:50],
        'method': request.method,
        'status': request.status,  # ä¿®æ­£ï¼šåº”è¯¥æ˜¯ status è€Œä¸æ˜¯ status_code
        'is_note_related': False,
        'type': 'OTHER'
    }

    # ğŸ¯ é‡ç‚¹ç›‘å¬ç¬”è®°åˆ—è¡¨æ¥å£
    if 'edith.xiaohongshu.com/api/sns/web/v1/search/notes' in url:
        result['type'] = 'NOTE_LIST_API'
        result['is_note_related'] = True
        print(f"\nğŸ”¥ å‘ç°å…³é”®æ¥å£ï¼ç¬”è®°åˆ—è¡¨API: {request.url}")
    elif '/search/' in url or 'keyword' in url:
        result['type'] = 'SEARCH'
        result['is_note_related'] = True
    elif '/feeds/' in url or 'note' in url:
        result['type'] = 'NOTE_LIST'
        result['is_note_related'] = True
    elif '/api/sns/web/v1/feed' in url:
        result['type'] = 'FEED_API'
        result['is_note_related'] = True
    elif '/api/sns/web/v1/search' in url:
        result['type'] = 'SEARCH_API'
        result['is_note_related'] = True

    return result

def extract_notes_from_request(request):
    """ä»è¯·æ±‚ä¸­æå–ç¬”è®°ä¿¡æ¯"""
    notes = []

    try:
        # ğŸ¯ ä¸“é—¨å¤„ç†ç¬”è®°åˆ—è¡¨API
        if 'edith.xiaohongshu.com/api/sns/web/v1/search/notes' in request.url:
            return extract_notes_from_search_api(request)

        # å°è¯•è§£æå“åº”æ•°æ®
        response_body = request.response.body

        if response_body:
            # å°è¯•JSONè§£æ
            try:
                data = json.loads(response_body)
                extracted = parse_json_for_notes(data)
                notes.extend(extracted)
            except json.JSONDecodeError:
                # å¦‚æœä¸æ˜¯JSONï¼Œå°è¯•å…¶ä»–è§£ææ–¹å¼
                print(f"âš ï¸ å“åº”ä¸æ˜¯JSONæ ¼å¼: {response_body[:200]}...")

        # ä»URLä¸­æå–ç¬”è®°ID
        note_ids = extract_note_ids_from_url(request.url)
        for note_id in note_ids:
            notes.append({
                'note_id': note_id,
                'title': f"ç¬”è®° {note_id}",
                'url': request.url,
                'capture_time': datetime.now().isoformat()
            })

    except Exception as e:
        print(f"âš ï¸ æå–ç¬”è®°ä¿¡æ¯å¤±è´¥: {str(e)}")

    return notes

def extract_notes_from_search_api(request):
    """ä¸“é—¨å¤„ç†æœç´¢ç¬”è®°APIçš„å“åº”"""
    notes = []

    try:
        response_body = request.response.body
        if not response_body:
            return notes

        print(f"ğŸ” è§£ææœç´¢APIå“åº” ({len(response_body)} å­—èŠ‚)")

        data = json.loads(response_body)
        print(f"ğŸ“Š APIå“åº”ç»“æ„: {list(data.keys()) if isinstance(data, dict) else type(data)}")

        # å°çº¢ä¹¦æœç´¢APIçš„å¸¸è§æ•°æ®ç»“æ„
        if isinstance(data, dict):
            if 'data' in data:
                notes.extend(parse_search_api_data(data['data']))
            elif 'items' in data:
                notes.extend(extract_notes_from_items(data['items']))
            elif 'notes' in data:
                notes.extend(extract_notes_from_items(data['notes']))

        print(f"âœ… ä»æœç´¢APIæå–åˆ° {len(notes)} ä¸ªç¬”è®°")

    except json.JSONDecodeError as e:
        print(f"âŒ JSONè§£æå¤±è´¥: {str(e)}")
    except Exception as e:
        print(f"âŒ æœç´¢APIè§£æå¤±è´¥: {str(e)}")

    return notes

def parse_search_api_data(data):
    """è§£ææœç´¢APIçš„dataå­—æ®µ"""
    notes = []

    if isinstance(data, dict):
        # æ£€æŸ¥å¸¸è§çš„æœç´¢APIç»“æ„
        if 'items' in data:
            notes.extend(extract_notes_from_items(data['items']))
        elif 'notes' in data:
            notes.extend(extract_notes_from_items(data['notes']))
        elif 'data' in data:  # åµŒå¥—ç»“æ„
            notes.extend(parse_search_api_data(data['data']))
    elif isinstance(data, list):
        notes.extend(extract_notes_from_items(data))

    return notes

def parse_json_for_notes(data):
    """ä»JSONæ•°æ®ä¸­è§£æç¬”è®°ä¿¡æ¯"""
    notes = []

    try:
        # å¸¸è§çš„ç¬”è®°æ•°æ®ç»“æ„
        if isinstance(data, dict):
            # æ£€æŸ¥å¸¸è§çš„ç¬”è®°å­—æ®µ
            if 'data' in data:
                notes.extend(extract_notes_from_data_field(data['data']))
            elif 'items' in data:
                notes.extend(extract_notes_from_items(data['items']))
            elif 'notes' in data:
                notes.extend(extract_notes_from_items(data['notes']))

    except Exception as e:
        print(f"âš ï¸ JSONè§£æå¤±è´¥: {str(e)}")

    return notes

def extract_notes_from_data_field(data):
    """ä»dataå­—æ®µä¸­æå–ç¬”è®°"""
    notes = []

    if isinstance(data, list):
        for item in data:
            if isinstance(item, dict) and ('note_id' in item or 'id' in item):
                notes.append(parse_note_item(item))
    elif isinstance(data, dict):
        if 'note_id' in data or 'id' in data:
            notes.append(parse_note_item(data))

    return notes

def extract_notes_from_items(items):
    """ä»itemsåˆ—è¡¨ä¸­æå–ç¬”è®°"""
    notes = []

    if isinstance(items, list):
        for item in items:
            if isinstance(item, dict):
                # æ£€æŸ¥æ˜¯å¦åŒ…å«ç¬”è®°ç›¸å…³ä¿¡æ¯
                if 'note_card' in item:
                    notes.append(parse_note_item(item['note_card']))
                elif 'note_id' in item or 'id' in item:
                    notes.append(parse_note_item(item))

    return notes

def parse_note_item(item):
    """è§£æå•ä¸ªç¬”è®°é¡¹"""
    note_id = item.get('note_id') or item.get('id') or 'unknown'
    title = item.get('title') or item.get('desc') or f"ç¬”è®° {note_id}"

    return {
        'note_id': note_id,
        'title': title,
        'url': f"https://www.xiaohongshu.com/explore/{note_id}",
        'author': item.get('user', {}).get('nickname', ''),
        'likes': item.get('interact_info', {}).get('liked_count', 0),
        'collects': item.get('interact_info', {}).get('collected_count', 0),
        'capture_time': datetime.now().isoformat(),
        'raw_data': item
    }

def extract_note_ids_from_url(url):
    """ä»URLä¸­æå–ç¬”è®°ID"""
    import re

    # å¸¸è§çš„ç¬”è®°IDæ¨¡å¼
    patterns = [
        r'/explore/([a-f0-9]+)',  # /explore/{note_id}
        r'/discovery/item/([a-f0-9]+)',  # /discovery/item/{note_id}
        r'noteId["\s]*[:=]["\s]*([a-f0-9]+)',  # JSONä¸­çš„noteId
    ]

    note_ids = []
    for pattern in patterns:
        matches = re.findall(pattern, url)
        note_ids.extend(matches)

    return list(set(note_ids))  # å»é‡

def save_captured_notes(notes):
    """ä¿å­˜æ•è·çš„ç¬”è®°ä¿¡æ¯ - ä½¿ç”¨æ–°çš„æ•°æ®ç»“æ„"""
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    filename = f'scripts/note_list_capture_{timestamp}.json'

    try:
        # ä½¿ç”¨æ–°çš„æ•°æ®ç»“æ„ä¿å­˜
        save_notes_to_json(notes, filename)
        print(f"ğŸ’¾ ç¬”è®°æ•°æ®å·²ä¿å­˜åˆ°: {filename}")

        # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        generate_summary_report(notes, filename.replace('.json', '_summary.txt'))

    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±è´¥: {str(e)}")

def generate_summary_report(notes, filename):
    """ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š - ä½¿ç”¨æ–°çš„æ•°æ®ç»“æ„"""
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            f.write("å°çº¢ä¹¦ç¬”è®°åˆ—è¡¨æŠ“å–ç»Ÿè®¡æŠ¥å‘Š\n")
            f.write("=" * 40 + "\n\n")
            f.write(f"æŠ“å–æ—¶é—´: {datetime.now().isoformat()}\n")
            f.write(f"ç¬”è®°æ€»æ•°: {len(notes)}\n\n")

            # ç»Ÿè®¡ä¿¡æ¯
            video_count = sum(1 for note in notes if note.is_video())
            image_count = sum(1 for note in notes if note.has_images())
            total_likes = sum(note.get_like_count() for note in notes)
            total_comments = sum(note.get_comment_count() for note in notes)
            total_collects = sum(note.get_collect_count() for note in notes)

            f.write("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:\n")
            f.write("-" * 20 + "\n")
            f.write(f"å›¾æ–‡ç¬”è®°: {len(notes) - video_count} ä¸ª\n")
            f.write(f"è§†é¢‘ç¬”è®°: {video_count} ä¸ª\n")
            f.write(f"åŒ…å«å›¾ç‰‡: {image_count} ä¸ª\n")
            f.write(f"æ€»ç‚¹èµæ•°: {total_likes}\n")
            f.write(f"æ€»è¯„è®ºæ•°: {total_comments}\n")
            f.write(f"æ€»æ”¶è—æ•°: {total_collects}\n\n")

            # æ˜¾ç¤ºå‰10ä¸ªç¬”è®°
            f.write("ğŸ“ å‰10ä¸ªç¬”è®°è¯¦æƒ…:\n")
            f.write("-" * 30 + "\n")
            for i, note in enumerate(notes[:10], 1):
                f.write(f"{i}. {note.title}\n")
                f.write(f"   ID: {note.note_id}\n")
                f.write(f"   ä½œè€…: {note.get_username()}\n")
                f.write(f"   ç±»å‹: {'è§†é¢‘' if note.is_video() else 'å›¾æ–‡'}\n")
                f.write(f"   äº’åŠ¨: ç‚¹èµ{note.get_like_count()} è¯„è®º{note.get_comment_count()} æ”¶è—{note.get_collect_count()}\n")
                f.write(f"   URL: {note.note_url}\n\n")

        print(f"ğŸ“Š ç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜åˆ°: {filename}")

    except Exception as e:
        print(f"âŒ ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {str(e)}")

def process_search_api_response(response_body):
    """å¤„ç†æœç´¢APIå“åº”æ•°æ® - ä½¿ç”¨æ–°çš„æ•°æ®ç»“æ„"""
    try:
        print(f"ğŸ” å¤„ç†APIå“åº” ({len(response_body)} å­—èŠ‚)")

        # DrissionPageä¼šè‡ªåŠ¨å°†JSONè½¬ä¸ºdict
        if isinstance(response_body, dict):
            data = response_body
        else:
            data = json.loads(response_body)

        print(f"ğŸ“Š å“åº”ç»“æ„: {list(data.keys()) if isinstance(data, dict) else type(data)}")

        # ä½¿ç”¨æ–°çš„æ•°æ®ç»“æ„è§£æ
        search_response = XiaohongshuSearchResponse.from_dict(data)
        notes = search_response.notes

        print(f"âœ… æå–åˆ° {len(notes)} ä¸ªç¬”è®°")

        # æ˜¾ç¤ºå‰å‡ ä¸ªç¬”è®°çš„æ‘˜è¦
        for i, note in enumerate(notes[:3]):
            print(f"\nğŸ“ ç¬”è®° {i+1} æ‘˜è¦:")
            print(note.format_summary())

        return notes

    except Exception as e:
        print(f"âŒ å¤„ç†APIå“åº”å¤±è´¥: {str(e)}")
        return []


if __name__ == "__main__":
    test_note_list_capture()